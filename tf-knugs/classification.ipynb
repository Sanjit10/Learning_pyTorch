{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of a classification neural network or model\n",
    "\n",
    "* **Input layer shape (in_features)**: This should be the same as the number of features in your dataset. \n",
    "For example, if you're working with images that are 28x28 pixels, your input layer should have 28*28 = 784 nodes.\n",
    "\n",
    "* **Hidden Layers**: The number and size of hidden layers are problem-specific. \n",
    "Start with one hidden layer with a size in the range of [16, 32, 64, 128, 256, 512, 1024]. \n",
    "You can add more layers if needed to extract more complex patterns or features. \n",
    "Deep networks can model complex tasks, but they also risk overfitting and require more data and computational power.\n",
    "\n",
    "* **Output layer shape (out_features)**: This should be the same as the number of classes in your dataset. \n",
    "For example, for a binary classification problem, you'd have 2 output nodes. \n",
    "For a 10-class problem like MNIST digit classification, you'd have 10 output nodes.\n",
    "\n",
    "* **Hidden layer activation function**: Common choices are the ReLU (Rectified Linear Unit), Tanh, and Sigmoid functions. \n",
    "ReLU is often preferred because it helps mitigate the vanishing gradients problem during backpropagation.\n",
    "\n",
    "* **Output layer activation function**: For multi-class classification, the softmax function is a common choice as it gives a probability distribution over the classes. \n",
    "For binary classification, a single output node with a sigmoid activation function is typically used.\n",
    "\n",
    "* **Loss function**: Cross entropy loss is used when there are 2 or more classes. \n",
    "It measures the dissimilarity between the predicted probability distribution and the actual distribution. \n",
    "For multi-class problems, you can use the NLL (Negative Log Likelihood) loss, which is suitable when your outputs are probability distribution estimates.\n",
    "\n",
    "* **Optimizer**: This is the algorithm used to update the weights of the network. \n",
    "Common choices are SGD (Stochastic Gradient Descent), Adam, and RMSProp. \n",
    "Adam is often preferred as it combines the advantages of two other extensions of SGD - AdaGrad and RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
