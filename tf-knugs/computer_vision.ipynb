{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "#Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# write device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = \"cpu\"\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a dataset fashion MNIST\n",
    "\n",
    "# Setup Training data\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root = 'tf-knugs/datasets',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'tf-knugs/datasets',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset= train_data,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset= test_data,\n",
    "    batch_size= BATCH_SIZE,\n",
    "    shuffle= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders:(<torch.utils.data.dataloader.DataLoader object at 0x7f2d3d0929d0>, <torch.utils.data.dataloader.DataLoader object at 0x7f2d3d3a1310>) \n",
      "Length of train_dataloader: 1875 batches of 32...\n",
      "Length of test_dataloader: 313 batches of 32...\n"
     ]
    }
   ],
   "source": [
    "print(f\"DataLoaders:{train_dataloader, test_dataloader} \")\n",
    "print(f\"Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...\")\n",
    "print(f\"Length of test_dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_dataloader)*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out whats inside the training data loader\n",
    "\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show sample\n",
    "# torch.manual_seed(42)\n",
    "class_names = train_data.classes\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_names) # one for every class\n",
    ")\n",
    "model_0.to(device) # keep model on CPU to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to time our expiriment\n",
    "\n",
    "Machine learning is very expirimental.\n",
    "\n",
    "Two main things we want ot keep track of is:\n",
    "1. Model's performance (loss, acc)etc\n",
    "2. Models spped or how fast it runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format).\n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_shape_device(*args):\n",
    "    \"\"\"\n",
    "    This function prints the shape, data type and device of the given tensors or numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    *args (torch tensor or numpy array): Variable length argument list of tensors or arrays.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for arg in args:\n",
    "        if torch.is_tensor(arg):\n",
    "            print(f\"Shape: {arg.shape} | Device: {arg.device}| Type : {arg.dtype} | torch tensor\")\n",
    "        elif isinstance(arg, np.ndarray):\n",
    "            print(f\"Shape: {arg.shape} | Device: CPU | Type : {arg.dtype} | numpy array\")\n",
    "        else:\n",
    "            print(\"Input type not supported. Please provide a torch tensor or a numpy array.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a training loop and trainin a model pn batches of data\n",
    "\n",
    "1. Loop throung epoch.\n",
    "2. Loop through training batches, perform training steps , calculate the train loss *per batch*.\n",
    "3. Loop through testing batches, perform tetsing steps, claculate the test loos *per batch*.\n",
    "4. Print out whats happening.\n",
    "5. Time is all (for analytics and fun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04efed7b9fe84bc297453c9d9c407fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.59039 | Test loss: 0.51016, Test acc: 82.04%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.47435 | Test loss: 0.52677, Test acc: 81.68%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.45367 | Test loss: 0.51264, Test acc: 83.00%\n",
      "\n",
      "Train time on cpu: 30.840 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "\n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                           end=train_time_end_on_gpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predicitions and get model results\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               dataloader: DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn\n",
    "):\n",
    "    \"\"\"Returns a dictonary condataining prediciton parameter of a module\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to evaluate\n",
    "        dataloader (DataLoader): a dataloader that loads data by batches\n",
    "        loss_fn (torch.nn.Module): function that calculates loss\n",
    "        accuracy_fn (_type_): function that claculates accuracy\n",
    "    \"\"\"\n",
    "    loss, acc = 0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in tqdm(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Make predicitons\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Accumulate the loss and accuracy values\n",
    "            acc += accuracy_fn(y_true= y, y_pred= y_pred.argmax(dim=1))\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "        #Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(dataloader)\n",
    "        acc /= len(dataloader)\n",
    "\n",
    "    return {\n",
    "        \"model_name\" : model.__class__.__name__,\n",
    "        \"model_loss\": loss.item(),\n",
    "        \"model_acc\": acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b65a7f0ac6941c8aedf0929af000d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'FashionMNISTModelV0', 'model_loss': 0.0010662442073225975, 'model_acc': 83.00718849840256}\n"
     ]
    }
   ],
   "source": [
    "# Use the function to calculate\n",
    "model0_results = eval_model(\n",
    "    model=model_0,\n",
    "    dataloader= test_dataloader,\n",
    "    accuracy_fn= accuracy_fn,\n",
    "    loss_fn= loss_fn\n",
    ")\n",
    "print(model0_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
